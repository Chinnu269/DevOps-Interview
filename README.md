# DevOps-Interview
>> COMMENT

-->The difference between `git pull` and `git fetch` lies in what they do with the remote changes.
----------------------------------------------------------------------------------------------------------------------

a) **`git fetch`:**
   - This command downloads new data from the remote repository, including new branches, commits, and tags.
   - It **does not** modify your working directory or merge any changes into your local branch. It simply updates your local copy of the remote branches (in `origin`, for example), so you can see what changes are available.
   - It's essentially a way to "check" for new updates without integrating them into your work.

   **Use case:** When you want to see if there are any updates on the remote before merging them into your local branch.

b) **`git pull`:**
   - This command is essentially a combination of `git fetch` followed by `git merge`.
   - It **downloads** changes from the remote repository (like `git fetch`) and **automatically merges** them into your current branch.
   - This means your working directory will be updated with any new commits from the remote branch you're tracking, which might result in a merge if there are changes in both your local and remote branches.

   **Use case:** When you want to automatically integrate the remote changes into your current branch, making your local branch up-to-date with the remote repository.

### Summary:
- `git fetch` only **fetches** the changes and leaves your working directory untouched.
- `git pull` **fetches and merges** the changes, updating your local branch with remote changes.


--> Maven  lifecycle and it's steps:
----------------------------------------------------------------------------------------------------------------------------------------------

Maven is a build automation tool which provided nice frame work to build the java based projects.It's organizes build process through predefined sequences knows as build lifecycles.

--> We have 3 build lifecycles in maven:

1. Default lifecycle: This is the primary lifecycle it is responsible for complete project deployment. It handles complete build process from validation to deployment.

Executing "mvn package" will run the validate, compile, test, and package phases sequentially. 

validate:  Checks project correctness and ensures all the informtion available
compile:  Compiles projects source code
test:  Runs unit test's using suitable testing frame work
package: packages the compiled code in to distributable format such as WAR,JAR files
verify:   Performs the checks to meet the quality criteria
install:  Install the package in the local repositry to use as dependency in other projects 
deploy: Copies the final projects into remote repository for sharing with other projects and developers. 

2. Clean lifecycle:  It is mainly focussed on project cleanup. It removes the files which are generated in previous builds and provides clean slate for new builds or new projects.

pre-clean: Performs actions required before the actual cleaning.​
clean: Removes files generated by previous builds.​
post-clean: Performs actions required after the cleaning process.

3. Site lifecycle: This lifecycle assists in creating site reports and generating project documentation.

pre-site: Performs actions required before site generation.​
Site: Generates the project's site documentation.​
post-site: Performs actions required after site generation.​
Site-deploy: Deploys the generated site documentation to a web server.​
sudo systemctl enable jenkins
sudo systemctl start jenkins

--> How to integrate github with jenkins:
------------------------------------------------------------------------------------------------------------------------------------------------------------------

1) Install the Necessary Plugins on Jenkins:

By default Github plugins wiill be installed on jenkins  but if there is no plugins installed.

Manage jenkins > Manage plugins, Go to 

Avaiable plugins and search for below mentioned plugins and install:

Git plugin: Enables Jenkins to interact with Git repositories.
GitHub plugin: Allows Jenkins to connect with GitHub.
GitHub Branch Source plugin (optional for organization-level integration).

Post the installation need to restart the jenkins.

2) Configure GitHub in Jenkins:

Dashboard > Managejenkins> Manage credintials
Select the appropriate domain or create a new one.
chood KIND as username and password if your using PAT(personal access token)
Save the credintials

3) Create a New Jenkins Job (or Pipeline):

On Jenkins dashboard, click New Item.

Choose Freestyle project or Pipeline depending on your needs.

Name your job and click OK.

4) Configure GitHub Repository in Jenkins Job:

In the Job configuration page scroll to the source code management and select GIT

Enter your GitHub repository URL (you can copy it from the GitHub repository page).

Example URL: https://github.com/username/repository-name.git

Under the credintials field select the credintials that you addded earlier 

In the Branch Specifier field, you can specify the branch you want Jenkins to monitor (e.g., */main)

5) Set Up Build Triggers
To automatically trigger Jenkins builds when changes are pushed to GitHub:

Scroll down to Build Triggers.

Select GitHub hook trigger for GITScm polling.

6) Set Up GitHub Webhook
Go to your GitHub repository page.

Click on Settings > Webhooks > Add webhook.

In the Payload URL, enter your Jenkins webhook URL. It should look like this:

perl
Copy
http://<jenkins-server>/github-webhook/
Set Content type to application/json.

In Which events would you like to trigger this webhook?, select Just the push event.

Click Add webhook.

7. Test the Integration
Make a push to your GitHub repository, either by committing changes or merging a pull request.

This should automatically trigger a Jenkins build.

Check Jenkins to see if the build has started as expected.


--> How to integrate sonar qube with jenkins:
-------------------------------------------------------------------------------------------------------------------------------------------------------
1)Install Sonar qube:
Download and Install sonar qube server on your server
Access the sonar qube dashboard through your webservers http://localhost:9000

2)Sonar qube scanner:
Download and install sonar qube scanner on your jenkins server
Ensure that the sonar-scanner command is avaible in your system's Path

3) Install sonar qube plugins in Jenkins:
In Jenkins,navigte t Manage jenkins > Manage plugins 
Under the avaible tab search for the Sonarqube scanner
select the plugin and click and  install with out restart

4) Configure Sonarqbe in jenkins:
Go to Manage Jenkins>Configure system
Scroll the Sonarqube servers section and click add sonarqube
Enter a name for your sonar qube server and provide sonar qube server URL http://localhost:9000
If authentication is requred we can enter credintials
Save configuration

5)Configure sonar qube scanner in Jenkins: 
In the same Configure System page, scroll to the SonarQube Scanner section.​
Click Add SonarQube Scanner.​
Provide a name for the scanner and specify the path to the sonar-scanner executable if it's not in the system PATH.​
Save the configuration.

6) Add SonarQube Analysis to Your Jenkins Pipeline:
In the jenkins pipeline or Jenkinsfile add a stage for the Sonar qube analysis

7. Run the Jenkins Pipeline:
Trigger a build in Jenkins.​
The pipeline will perform the SonarQube analysis during the specified stage.​

8. View Analysis Results:
After the build completes, navigate to your SonarQube dashboard to view the code quality analysis results.

Difference between sonarqube and sonar scanner: Sonar scanner handles actual code analysis, it identifies and provides errors and bugs  to the sonra qube which helps you to provide visulaization and results.Sonar qube is a centralized platform where you can see all the code quality analysis results

--> How to link nexus artifactory with jenkins pipeline:
------------------------------------------------------------------------------------------------------------------------------------------

1) Install nexus artifact uploader plugin:

This plugin facilitates the uploading of build artifacts to Nexus repositories.

Manage jenkins > Mnage plugins 

Available plugins then search for nexus artifact uploader plugin and install the plugin with out restart

2) Repository connector plugin:

This plugin helps the jenkins to retrive or deploy artificats to the respositories like nexus

Manage jenkins > Mnage plugins

Available plugins then search for Repository connector plugin  and install the plugin with out restart

3) Configure Maven Settings (Optional but Recommended):

Manage Maven's settings.xml:
Use the Config File Provider Plugin to manage Maven's settings.xml centrally, enhancing security and consistency.
Implementation Steps:

Navigate to Manage Jenkins > Managed Files.

Create a new file with the necessary Nexus repository configurations.
In your Jenkins pipeline, use the withMaven step to reference this configuration

4) Setup jenkins pipeline to interact with nexus:

Build stage: compile your code using the maven or your specific build tool
Code check: Check the code quality using the sonar qube
Test stage: perform the unit tests to check the code quality
artifact stage: package your appication into artificat like .JAR .WAR file
deploy stage: upload the artifacts

5) Securely Manage Nexus Credentials:

Use Jenkins Credentials Plugin:
Store Nexus credentials securely within Jenkins.
Steps:
Navigate to Manage Jenkins > Manage Credentials.
Add a new credential with your Nexus username and password or token.

Reference this credential in your pipeline using the credentialsId parameter.

6. Post-Build Nexus Tasks (Optional):
Execute Nexus Tasks:
Use the Nexus Task Runner Plugin to perform tasks like repository indexing after builds.
Implementation Steps:
In Nexus, define the tasks you wish to execute.
In Jenkins, configure post-build actions to run these Nexus tasks.
Reference: For more details, refer to the Nexus Task Runner Plugin documentation.

--> How to read properties while starting a job in jenkins:
--------------------------------------------------------------------------------------------------------------------

In Jenkins, if you want to read and pass properties (such as parameters or environment variables) when starting a job, there are several ways to do it, depending on how you are invoking the job. 
Here are the common methods to read and pass properties while starting a job in Jenkins:

Parameterized Jobs: Use parameters to pass properties when starting a job manually.
Environment Variables: Use predefined or custom environment variables in your job or pipeline.
Jenkins Pipeline: Use params or input to read and pass properties.you can define paraketers in the Jenkins file and access them with in the pipeline script
Properties File: Load external properties from a file and use them in your job or pipeline.

--> which are plugins are needed while running a pipeline:
-------------------------------------------------------------------------------------------------------

when running a jenkins pipeline we will use several plugins depending upon the pipeline which we are running and the features we are using. The most commonly used plugins are 

1) Githubplugin
2) stage view plugin
3) Sonarqube plugin
4) Blueocean plugin
5) nexus artifact uploader
5) reposiory connector
6) Groovy plugin
7) credintials binding plugin
8) pipeline plugin
9) SSH agent plugin
10) AWS steps
11) Docker Pipeline Plugin
12) Junit plugin

---> what is meant by green guilds:
---------------------------------------------------------------------------------------------

In devops green build means status of the build and depolyment all the checks have passed successfully.
The pipeline has completed with out any errors and all automted process completed successfully.

Example:
Imagine a Jenkins pipeline:

The pipeline includes steps like:
Compile: Code is compiled successfully (green).
Unit Tests: All unit tests pass (green).
Integration Tests: Integration tests run and pass (green).
Deployment: Code is deployed successfully to staging (green).

When the build status is green, developers know that the changes they've made are solid and that they are ready for production deployment if desired.
A green build in DevOps signifies that the entire automated build and test pipeline has executed successfully, and the code is in a healthy, deployable state. 
It’s a positive indication of quality and stability in the software development lifecycle.

--> what are the steps for developing CI/CD pipelines in Jenkins:
--------------------------------------------------------------------------------------------------------------------------------------

The CI/CD pipeline automates the process of the Build,test and deployment of the application. There are multiple steps and stages are involved in this process.

Creating a CI/CD pipeline in Jenkins involves setting up the Jenkins server, integrating with version control, 
defining a pipeline script (using Declarative or Scripted Pipelines), configuring build triggers, handling credentials, and automating notifications.
 Over time, you can improve your pipeline by optimizing performance, adding stages (such as static code analysis or security scans), and enhancing your deployment processes.
 The pipeline serves as the backbone of your automation in the DevOps lifecycle, ensuring efficient and consistent delivery of software.

Steps: 

Code Commit: Developers submit their code changes to a shared version control repository, such as Git.​

Build: The pipeline automatically compiles the code, resolves dependencies, and creates executable artifacts.​
OutstandingStar.com

Automated Testing: Automated tests, including unit, integration, and functional tests, are executed to verify the correctness and quality of the code.​
ByteGoblin

Deployment:
Staging Deployment: If tests pass, the application is deployed to a staging environment that mirrors production for further validation.​
Production Deployment: Following successful staging validation, the application is automatically deployed to the production environment, making it available to end-users.​
Monitoring and Feedback: Post-deployment, the application’s performance is continuously monitored to detect issues, gather user feedback, and inform future improvements.

--> Any log frame work integratiion tool:
-------------------------------------------------------------------------------------------------
I have used promethus as log moniotoring and alerting tool.

Prometheus: 

Provides a robust query language called PROMQL to extract and manipulate time-series data efficiently.

Steps to Integrate Prometheus with Jenkins:

1) Install the Prometheus Metrics Plugin in Jenkins:
Navigate to "Manage Jenkins" > "Manage Plugins".​
In the "Available" tab, search for "Prometheus Metrics Plugin".​
Install the plugin and restart Jenkins if prompted.​

2) Configure the Plugin:

Go to "Manage Jenkins" > "Configure System".​

Locate the "Prometheus Metrics" section.​
Enable the metrics endpoint and set the desired "Metrics Collection Period".​
Save the configuration.​

3) Set Up Prometheus to Scrape Jenkins Metrics:

In your prometheus.yml configuration file, add a new scrape job:​

yaml
Copy
scrape_configs:
  - job_name: 'jenkins'
    metrics_path: '/prometheus'
    static_configs:
      - targets: ['<JENKINS_HOSTNAME>:<JENKINS_PORT>']

Replace <JENKINS_HOSTNAME> and <JENKINS_PORT> with your Jenkins server's hostname and port number.

Restart Prometheus to apply the changes.​

4) Verify the Integration:

Access Prometheus's web interface (typically at http://<PROMETHEUS_HOST>:9090).​

Navigate to the "Targets" page (http://<PROMETHEUS_HOST>:9090/targets) to ensure Prometheus is successfully scraping metrics from Jenkins.​
By following these steps, you can effectively monitor Jenkins builds and system performance using Prometheus, gaining valuable insights into your CI/CD pipeline's health and efficiency.​

--> What is DMZ(Demelitarized zone)
-----------------------------------------------------------------------
In network DMZ is also known as permiter network or screened subnet, It is a logical or physical subnet work which separates you office local network from the un trusted external network typically pubic network.

The primary goal of DMZ is to provide the extra layer of security to the internal network by isolating the external facing services.This will ensure even if the public facing servers are compromised the internal network remains protected.


--> Common Services Hosted in a DMZ:
---------------------------------------------------

Web Servers: Handle HTTP/HTTPS requests from users on the internet.​
Email Servers: Manage incoming and outgoing email communications.​
DNS Servers: Resolve domain names to IP addresses for clients.​
FTP Servers:  Facilitate file transfers between systems.​
Proxy Servers:  Act as intermediaries between users and the internet, enhancing security and privacy.​

Benefits of Implementing a DMZ:

Access Control: A DMZ provides controlled access to services from external networks, reducing the risk of unauthorized access to the internal network. ​
Network Reconnaissance Prevention: By isolating public-facing servers, a DMZ prevents attackers from conducting reconnaissance on internal network structures, adding a layer of obscurity. ​
Protection Against IP Spoofing: A DMZ can help detect and mitigate IP spoofing attempts, where attackers impersonate trusted devices to gain unauthorized access. ​


Design Architectures of DMZs:

Single Firewall DMZ: Utilizes one firewall with at least three network interfaces—connecting to the external network (internet), the internal network, and the DMZ itself. This setup requires careful configuration of firewall rules to control traffic flow between networks. ​
Dual Firewall DMZ: Employs two firewalls, with the DMZ situated between them. The first firewall controls external traffic to the DMZ, while the second manages traffic from the DMZ to the internal network. This configuration provides enhanced security by requiring an attacker to breach two security barriers to access the internal network. ​
By implementing a DMZ, organizations can effectively safeguard their internal networks while providing necessary services to external users, balancing accessibility with security.​


--> How to connect to external URL's:

Connecting external URL's means making HTTP requests from your application to external servers to retrieve or send data.This can be done in many ways depending upon the programming language and the tools we are using.

1. Using JavaScript in Web Browsers:
 -Fetch API: The Fetch API provides a modern, promise-based approach to making HTTP requests.This method is straightforward and widely supported in modern browsers.
-XMLHttpRequest: For compatibility with older browsers, you can use the XMLHttpRequest object.
-jQuery AJAX: If your project includes jQuery, you can utilize its AJAX methods for simplicity.

2. Using Node.js:
http/https Module: Node.js provides built-in modules to handle HTTP requests.This method offers fine-grained control over HTTP requests.

3. Using Python:
Requests Library: Python's requests library offers a user-friendly interface for making HTTP requests.
The requests library simplifies request creation and response handling.

4. Using cURL in Command Line:
cURL Tool: cURL is a command-line tool for transferring data with URLs.

COMMENT























